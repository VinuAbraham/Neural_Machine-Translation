{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "id": "H5a8hXKKhjv7",
    "outputId": "fb9fcc02-fb77-432b-81e5-f1e41e44bd17"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gzip\n",
    "import codecs\n",
    "import re\n",
    "import time\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.contrib.seq2seq import TrainingHelper, GreedyEmbeddingHelper, BasicDecoder, dynamic_decode\n",
    "from tensorflow.contrib.seq2seq import BahdanauAttention, AttentionWrapper, sequence_loss\n",
    "from tensorflow.contrib.rnn import GRUCell, DropoutWrapper\n",
    "TOKEN_GO = '<GO>'\n",
    "TOKEN_EOS = '<EOS>'\n",
    "TOKEN_PAD = '<PAD>'\n",
    "TOKEN_UNK = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9_PgSKUaEYQ"
   },
   "outputs": [],
   "source": [
    "import codecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JbyvNTuOiaLt"
   },
   "outputs": [],
   "source": [
    "maldata=[]\n",
    "endata=[]\n",
    "with open('/home/shiva/Desktop/training.ml-en.en',errors='ignore') as enfile:\n",
    "    for li in enfile:\n",
    "        endata.append(li)\n",
    "with open('/home/shiva/Desktop/training.ml-en.ml') as malfile:\n",
    "    for li in malfile:\n",
    "        maldata.append(li)\n",
    "        \n",
    "mtdata = pd.DataFrame({'MAL':maldata,'EN':endata})\n",
    "mtdata['MAL_len'] = mtdata['MAL'].apply(lambda x: len(x.split(' ')))\n",
    "mtdata['EN_len'] = mtdata['EN'].apply(lambda x: len(x.split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTXI_uBz_mw9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAiyByyzZb15"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qxarm1BVi2Je"
   },
   "outputs": [],
   "source": [
    "mtdata_mal = []\n",
    "for ml in mtdata.MAL:\n",
    "    mtdata_mal.append(ml)\n",
    "mtdata_en = []\n",
    "for en in mtdata.EN:\n",
    "    mtdata_en.append(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZZQrFDhi2-T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zomiXxVW2mvj"
   },
   "outputs": [],
   "source": [
    "def count_words(words_dict, text):\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in words_dict:\n",
    "                words_dict[word] = 1\n",
    "            else:\n",
    "                words_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "nVOE4d2T3GEh",
    "outputId": "5a2cfb13-9d0a-4b55-d41b-e584f3f8b2e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Malayalam words in Vocabulary: 29715\n",
      "Total English words in Vocabulary 31863\n"
     ]
    }
   ],
   "source": [
    "word_counts_dict_ml = {}\n",
    "word_counts_dict_en = {}\n",
    "count_words(word_counts_dict_ml, mtdata_mal)\n",
    "count_words(word_counts_dict_en, mtdata_en)\n",
    "#print(mtdata_en)\n",
    "            \n",
    "print(\"Total Malayalam words in Vocabulary:\", len(word_counts_dict_ml))\n",
    "print(\"Total English words in Vocabulary\", len(word_counts_dict_en))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xK3O68b3RdV"
   },
   "outputs": [],
   "source": [
    "def build_word_vector_matrix(vector_file):\n",
    "    embedding_index = {}\n",
    "    with codecs.open(vector_file, 'r', 'utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            sr = line.split()\n",
    "            if(len(sr)<26):\n",
    "                continue\n",
    "            word = sr[0]\n",
    "            embedding = np.asarray(sr[1:], dtype='float32')\n",
    "            embedding_index[word] = embedding\n",
    "        #print(\"sr :\",sr,\"i:\",i)\n",
    "    return embedding_index\n",
    "embeddings_index = build_word_vector_matrix('/home/shiva/Desktop/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8oa_NaB3-ZA"
   },
   "outputs": [],
   "source": [
    "def build_word2id_mapping(word_counts_dict):\n",
    "    word2int = {} \n",
    "    count_threshold =0\n",
    "    value = 0\n",
    "    for word, count in word_counts_dict.items():\n",
    "        if count >= count_threshold or word in embeddings_index:\n",
    "            word2int[word] = value\n",
    "            value += 1\n",
    "\n",
    "\n",
    "    special_codes = [TOKEN_UNK,TOKEN_PAD,TOKEN_EOS,TOKEN_GO]   \n",
    "\n",
    "    for code in special_codes:\n",
    "        word2int[code] = len(word2int)\n",
    "\n",
    "    int2word = {}\n",
    "    for word, value in word2int.items():\n",
    "        int2word[value] = word\n",
    "    return word2int,int2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdrHWr073_fX"
   },
   "outputs": [],
   "source": [
    "def build_embeddings(word2int):\n",
    "    embedding_dim = 50\n",
    "    nwords = len(word2int)\n",
    "\n",
    "    word_emb_matrix = np.zeros((nwords, embedding_dim), dtype=np.float32)\n",
    "    for word, i in word2int.items():\n",
    "        if word in embeddings_index:\n",
    "            word_emb_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "            word_emb_matrix[i] = new_embedding\n",
    "    return word_emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "pqeVv30y8uG5",
    "outputId": "19e7407d-bddb-4c65-cb2e-7113e44beaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of malayalam word embeddings:  29719\n",
      "Length of english word embeddings:  31867\n"
     ]
    }
   ],
   "source": [
    "ml_word2int,ml_int2word = build_word2id_mapping(word_counts_dict_ml)\n",
    "en_word2int,en_int2word = build_word2id_mapping(word_counts_dict_en)\n",
    "ml_embeddings_matrix = build_embeddings(ml_word2int)\n",
    "en_embeddings_matrix = build_embeddings(en_word2int)\n",
    "print(\"Length of malayalam word embeddings: \", len(ml_embeddings_matrix))\n",
    "print(\"Length of english word embeddings: \", len(en_embeddings_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8yhgxW1D9BRJ"
   },
   "outputs": [],
   "source": [
    "def convert_sentence_to_ids(text, word2int, eos=False):\n",
    "    wordints = []\n",
    "    word_count = 0\n",
    "    for sentence in text:\n",
    "        sentence2ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in word2int:\n",
    "                sentence2ints.append(word2int[word])\n",
    "            else:\n",
    "                sentence2ints.append(word2int[TOKEN_UNK])\n",
    "        if eos:\n",
    "            sentence2ints.append(word2int[TOKEN_EOS])\n",
    "        wordints.append(sentence2ints)\n",
    "    return wordints, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gN0N58DS-u04"
   },
   "outputs": [],
   "source": [
    "id_ml, word_count_ml = convert_sentence_to_ids(mtdata_mal, ml_word2int)\n",
    "id_en, word_count_en = convert_sentence_to_ids(mtdata_en, en_word2int, eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9zPiZHAC_TnB"
   },
   "outputs": [],
   "source": [
    "def unknown_tokens(sentence, word2int):\n",
    "    unk_token_count = 0\n",
    "    for word in sentence:\n",
    "        if word == word2int[TOKEN_UNK]:\n",
    "            unk_token_count += 1\n",
    "    return unk_token_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ci14Am_IAt4P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ddaki8eiBgeH",
    "outputId": "3347d3f8-5890-4bf9-ad13-51ca536e8173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of filtered malayalam/english sentences:  24972 24972\n"
     ]
    }
   ],
   "source": [
    "en_filtered = []\n",
    "ml_filtered = []\n",
    "max_en_length = int(mtdata.EN_len.max())\n",
    "max_ml_length = int(mtdata.MAL_len.max())\n",
    "min_length = 4\n",
    "unknown_token_en_limit = 10\n",
    "unknown_token_ml_limit = 10\n",
    "\n",
    "for count,text in enumerate(id_en):\n",
    "    unknown_token_en = unknown_tokens(id_en[count],en_word2int)\n",
    "    unknown_token_ml = unknown_tokens(id_ml[count],ml_word2int)\n",
    "    en_len = len(id_en[count])\n",
    "    ml_len = len(id_ml[count])\n",
    "    if( (unknown_token_en>unknown_token_en_limit) or (unknown_token_ml>unknown_token_ml_limit) or \n",
    "       (en_len<min_length) or (ml_len<min_length) ):\n",
    "        continue\n",
    "    ml_filtered.append(id_ml[count])\n",
    "    en_filtered.append(id_en[count])\n",
    "print(\"Length of filtered malayalam/english sentences: \", len(ml_filtered), len(en_filtered) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QI09w7FzF2NW"
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    inputs_data = tf.placeholder(tf.int32, [None, None], name='input_data')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    dropout_probs = tf.placeholder(tf.float32, name='dropout_probs')\n",
    "    en_len = tf.placeholder(tf.int32, (None,), name='en_len')\n",
    "    max_en_len = tf.reduce_max(en_len, name='max_en_len')\n",
    "    ml_len = tf.placeholder(tf.int32, (None,), name='ml_len')\n",
    "    return inputs_data, targets, learning_rate, dropout_probs, en_len, max_en_len, ml_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgxsUkrBLcIK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R_63YJ8rLfLP"
   },
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, word2int, batch_size):\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    decoding_input = tf.concat([tf.fill([batch_size, 1], word2int[TOKEN_GO]), ending], 1)\n",
    "    return decoding_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "ExbXeyByQTFn",
    "outputId": "38283eff-312c-4a0b-c99d-1cc8a1265db1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OgxDia_VMXtn"
   },
   "outputs": [],
   "source": [
    "def get_rnn_cell(rnn_cell_size,dropout_prob):\n",
    "    rnn_c = GRUCell(rnn_cell_size)\n",
    "    rnn_c = DropoutWrapper(rnn_c, input_keep_prob = dropout_prob)\n",
    "    return rnn_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWKgVmZIOeIv"
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_cell_size, sequence_len, n_layers, rnn_inputs, dropout_prob):\n",
    "    for l in range(n_layers):\n",
    "        with tf.variable_scope('encoding_l_{}'.format(l)):\n",
    "            rnn_fw = get_rnn_cell(rnn_cell_size,dropout_prob)\n",
    "            rnn_bw = get_rnn_cell(rnn_cell_size,dropout_prob)\n",
    "            encoding_output, encoding_state = tf.nn.bidirectional_dynamic_rnn(rnn_fw, rnn_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_len,\n",
    "                                                                    dtype=tf.float32)\n",
    "    encoding_output = tf.concat(encoding_output,2)\n",
    "    return encoding_output, encoding_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1drJUXY7RXX8"
   },
   "outputs": [],
   "source": [
    "def training_decoding_layer(decoding_embed_input, en_len, decoding_cell, initial_state, op_layer, \n",
    "                            v_size, max_en_len):\n",
    "    helper = TrainingHelper(inputs=decoding_embed_input,sequence_length=en_len, time_major=False)\n",
    "    dec = BasicDecoder(decoding_cell,helper,initial_state,op_layer) \n",
    "    logits, _, _ = dynamic_decode(dec,output_time_major=False,impute_finished=True, \n",
    "                                  maximum_iterations=max_en_len)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhO8XhnKVZ2r"
   },
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, decoding_cell, initial_state, op_layer,\n",
    "                             max_en_len, batch_size):\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    inf_helper = GreedyEmbeddingHelper(embeddings,start_tokens,end_token)\n",
    "    inf_decoder = BasicDecoder(decoding_cell,inf_helper,initial_state,op_layer)       \n",
    "    inf_logits, _, _ = dynamic_decode(inf_decoder,output_time_major=False,impute_finished=True,\n",
    "                                                            maximum_iterations=max_en_len)\n",
    "    return inf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7RDSvqIVd0w"
   },
   "outputs": [],
   "source": [
    "def decoding_layer(decoding_embed_inp, embeddings, encoding_op, encoding_st, v_size, ml_len, \n",
    "                   en_len,max_en_len, rnn_cell_size, word2int, dropout_prob, batch_size, n_layers):\n",
    "    \n",
    "    for l in range(n_layers):\n",
    "        with tf.variable_scope('dec_rnn_layer_{}'.format(l)):\n",
    "            gru = tf.contrib.rnn.GRUCell(rnn_len)\n",
    "            decoding_cell = tf.contrib.rnn.DropoutWrapper(gru,input_keep_prob = dropout_prob)\n",
    "    out_l = Dense(v_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attention = BahdanauAttention(rnn_cell_size, encoding_op,ml_len,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "    decoding_cell =  AttentionWrapper(decoding_cell,attention,rnn_len)\n",
    "    attention_zero_state = decoding_cell.zero_state(batch_size , tf.float32 )\n",
    "    attention_zero_state = attention_zero_state.clone(cell_state = encoding_st[0])\n",
    "    with tf.variable_scope(\"decoding_layer\"):\n",
    "        logits_tr = training_decoding_layer(decoding_embed_inp, \n",
    "                                                  en_len, \n",
    "                                                  decoding_cell, \n",
    "                                                  attention_zero_state,\n",
    "                                                  out_l,\n",
    "                                                  v_size, \n",
    "                                                  max_en_len)\n",
    "    with tf.variable_scope(\"decoding_layer\", reuse=True):\n",
    "        logits_inf = inference_decoding_layer(embeddings,  \n",
    "                                                    word2int[TOKEN_GO], \n",
    "                                                    word2int[TOKEN_EOS],\n",
    "                                                    decoding_cell, \n",
    "                                                    attention_zero_state, \n",
    "                                                    out_l,\n",
    "                                                    max_en_len,\n",
    "                                                    batch_size)\n",
    "\n",
    "    return logits_tr, logits_inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0hp3Nt6YV0Cc"
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_en_data, dropout_prob, ml_len, en_len, max_en_len, \n",
    "                  v_size, rnn_cell_size, n_layers, word2int_en, batch_size):\n",
    "    \n",
    "    input_word_embeddings = tf.Variable(ml_embeddings_matrix, name=\"input_word_embeddings\")\n",
    "    encoding_embed_input = tf.nn.embedding_lookup(input_word_embeddings, input_data)\n",
    "    encoding_op, encoding_st = encoding_layer(rnn_cell_size, ml_len, n_layers, encoding_embed_input, dropout_prob)\n",
    "    \n",
    "    decoding_input = process_encoding_input(target_en_data, word2int_en, batch_size)\n",
    "    decoding_embed_input = tf.nn.embedding_lookup(en_embeddings_matrix, decoding_input)\n",
    "    \n",
    "    tr_logits, inf_logits  = decoding_layer(decoding_embed_input, \n",
    "                                                        en_embeddings_matrix,\n",
    "                                                        encoding_op,\n",
    "                                                        encoding_st, \n",
    "                                                        v_size, \n",
    "                                                        ml_len, \n",
    "                                                        en_len, \n",
    "                                                        max_en_len,\n",
    "                                                        rnn_cell_size, \n",
    "                                                        word2int_en, \n",
    "                                                        dropout_prob, \n",
    "                                                        batch_size,\n",
    "                                                        n_layers)\n",
    "    \n",
    "    return tr_logits, inf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ly-LCP5JWFIl"
   },
   "outputs": [],
   "source": [
    "def pad_sentences(sentences_batch,word2int):\n",
    "    max_sentence = max([len(sentence) for sentence in sentences_batch])\n",
    "    return [sentence + [word2int[TOKEN_PAD]] * (max_sentence - len(sentence)) for sentence in sentences_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFENBUzzWItS"
   },
   "outputs": [],
   "source": [
    "def get_batches(en_text, ml_text, batch_size):\n",
    "    for batch_idx in range(0, len(ml_text)//batch_size):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        en_batch = en_text[start_idx:start_idx + batch_size]\n",
    "        ml_batch = ml_text[start_idx:start_idx + batch_size]\n",
    "        pad_en_batch = np.array(pad_sentences(en_batch, en_word2int))\n",
    "        pad_ml_batch = np.array(pad_sentences(ml_batch,ml_word2int))\n",
    "\n",
    "        pad_en_lens = []\n",
    "        for en_b in pad_en_batch:\n",
    "            pad_en_lens.append(len(en_b))\n",
    "        \n",
    "        pad_ml_lens = []\n",
    "        for ml_b in pad_ml_batch:\n",
    "            pad_ml_lens.append(len(ml_b))\n",
    "        \n",
    "        yield pad_en_batch, pad_ml_batch, pad_en_lens, pad_ml_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZ-VCR83WM6y"
   },
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "batch_size = 64\n",
    "rnn_len = 256\n",
    "n_layers = 2\n",
    "lr = 0.005\n",
    "dr_prob = 0.75\n",
    "logs_path='/home/shiva/Desktop/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "colab_type": "code",
    "id": "DY1gPtnBWeY0",
    "outputId": "fcc60c2b-52e5-4b98-bad7-d08887c7a74b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1104 14:29:19.776827 140271006725952 deprecation.py:323] From <ipython-input-17-eb9e23340fcf>:2: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W1104 14:29:19.781417 140271006725952 deprecation.py:323] From <ipython-input-18-f0b8f753fd15>:9: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "W1104 14:29:19.782439 140271006725952 deprecation.py:323] From /home/shiva/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W1104 14:29:20.524904 140271006725952 deprecation.py:506] From /home/shiva/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1104 14:29:20.538990 140271006725952 deprecation.py:506] From /home/shiva/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:564: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1104 14:29:20.560193 140271006725952 deprecation.py:506] From /home/shiva/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:574: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1104 14:29:20.790086 140271006725952 deprecation.py:323] From /home/shiva/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    input_data, targets, learning_rate, dropout_probs, en_len, max_en_len, ml_len = model_inputs()\n",
    "\n",
    "    logits_tr, logits_inf = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      dropout_probs,   \n",
    "                                                      ml_len,\n",
    "                                                      en_len,\n",
    "                                                      max_en_len,\n",
    "                                                      len(en_word2int)+1,\n",
    "                                                      rnn_len, \n",
    "                                                      n_layers, \n",
    "                                                      en_word2int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    logits_tr = tf.identity(logits_tr.rnn_output, 'logits_tr')\n",
    "    logits_inf = tf.identity(logits_inf.sample_id, name='predictions')\n",
    "    \n",
    "    seq_masks = tf.sequence_mask(en_len, max_en_len, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        tr_cost = sequence_loss(logits_tr,targets,seq_masks)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        gradients = optimizer.compute_gradients(tr_cost)\n",
    "        capped_gradients = [(tf.clip_by_value(gradient, -5., 5.), var) for gradient, var in gradients \n",
    "                        if gradient is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    tf.summary.scalar(\"cost\", tr_cost)\n",
    "print(\"Graph created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "T5QXjkUeWf7r",
    "outputId": "f3651370-ae84-4541-8116-a1fbe210e05b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Epoch   1/300 Batch   20/390 - Batch Loss:  5.708, seconds: 57.11\n",
      "Average loss: 5.035\n",
      "Saving model\n",
      "** Epoch   1/300 Batch   40/390 - Batch Loss:  2.492, seconds: 59.97\n",
      "Average loss: 2.575\n",
      "Saving model\n",
      "** Epoch   1/300 Batch   60/390 - Batch Loss:  2.515, seconds: 104.34\n",
      "Average loss: 2.484\n",
      "Saving model\n",
      "** Epoch   2/300 Batch   20/390 - Batch Loss:  2.497, seconds: 57.02\n",
      "Average loss: 2.395\n",
      "Saving model\n",
      "** Epoch   2/300 Batch   40/390 - Batch Loss:  2.167, seconds: 59.94\n",
      "Average loss: 2.267\n",
      "Saving model\n",
      "** Epoch   2/300 Batch   60/390 - Batch Loss:  2.254, seconds: 105.32\n",
      "Average loss: 2.253\n",
      "Saving model\n",
      "** Epoch   3/300 Batch   20/390 - Batch Loss:  2.377, seconds: 57.04\n",
      "Average loss: 2.279\n",
      "No Improvement.\n",
      "** Epoch   3/300 Batch   40/390 - Batch Loss:  2.058, seconds: 60.17\n",
      "Average loss: 2.145\n",
      "Saving model\n",
      "** Epoch   3/300 Batch   60/390 - Batch Loss:  2.120, seconds: 105.32\n",
      "Average loss: 2.119\n",
      "Saving model\n",
      "** Epoch   4/300 Batch   20/390 - Batch Loss:  2.247, seconds: 57.34\n",
      "Average loss: 2.16\n",
      "No Improvement.\n",
      "** Epoch   4/300 Batch   40/390 - Batch Loss:  1.943, seconds: 61.32\n",
      "Average loss: 2.002\n",
      "Saving model\n",
      "** Epoch   4/300 Batch   60/390 - Batch Loss:  1.978, seconds: 105.61\n",
      "Average loss: 1.964\n",
      "Saving model\n",
      "** Epoch   5/300 Batch   20/390 - Batch Loss:  2.080, seconds: 56.95\n",
      "Average loss: 1.998\n",
      "No Improvement.\n",
      "** Epoch   5/300 Batch   40/390 - Batch Loss:  1.779, seconds: 59.93\n",
      "Average loss: 1.824\n",
      "Saving model\n",
      "** Epoch   5/300 Batch   60/390 - Batch Loss:  1.789, seconds: 104.82\n",
      "Average loss: 1.759\n",
      "Saving model\n",
      "** Epoch   6/300 Batch   20/390 - Batch Loss:  1.862, seconds: 56.99\n",
      "Average loss: 1.787\n",
      "No Improvement.\n",
      "** Epoch   6/300 Batch   40/390 - Batch Loss:  1.609, seconds: 60.28\n",
      "Average loss: 1.647\n",
      "Saving model\n",
      "** Epoch   6/300 Batch   60/390 - Batch Loss:  1.617, seconds: 104.31\n",
      "Average loss: 1.607\n",
      "Saving model\n",
      "** Epoch   7/300 Batch   20/390 - Batch Loss:  1.716, seconds: 56.55\n",
      "Average loss: 1.647\n",
      "No Improvement.\n",
      "** Epoch   7/300 Batch   40/390 - Batch Loss:  1.482, seconds: 60.16\n",
      "Average loss: 1.519\n",
      "Saving model\n",
      "** Epoch   7/300 Batch   60/390 - Batch Loss:  1.501, seconds: 104.90\n",
      "Average loss: 1.504\n",
      "Saving model\n",
      "** Epoch   8/300 Batch   20/390 - Batch Loss:  1.572, seconds: 56.07\n",
      "Average loss: 1.515\n",
      "No Improvement.\n",
      "** Epoch   8/300 Batch   40/390 - Batch Loss:  1.402, seconds: 59.81\n",
      "Average loss: 1.444\n",
      "Saving model\n",
      "** Epoch   8/300 Batch   60/390 - Batch Loss:  1.436, seconds: 104.54\n",
      "Average loss: 1.42\n",
      "Saving model\n",
      "** Epoch   9/300 Batch   20/390 - Batch Loss:  1.468, seconds: 56.46\n",
      "Average loss: 1.422\n",
      "No Improvement.\n",
      "** Epoch   9/300 Batch   40/390 - Batch Loss:  1.328, seconds: 60.59\n",
      "Average loss: 1.364\n",
      "Saving model\n",
      "** Epoch   9/300 Batch   60/390 - Batch Loss:  1.354, seconds: 111.19\n",
      "Average loss: 1.352\n",
      "Saving model\n",
      "** Epoch  10/300 Batch   20/390 - Batch Loss:  1.416, seconds: 57.97\n",
      "Average loss: 1.374\n",
      "No Improvement.\n",
      "** Epoch  10/300 Batch   40/390 - Batch Loss:  1.289, seconds: 60.10\n",
      "Average loss: 1.306\n",
      "Saving model\n",
      "** Epoch  10/300 Batch   60/390 - Batch Loss:  1.272, seconds: 108.38\n",
      "Average loss: 1.28\n",
      "Saving model\n",
      "** Epoch  11/300 Batch   20/390 - Batch Loss:  1.350, seconds: 56.73\n",
      "Average loss: 1.322\n",
      "No Improvement.\n",
      "** Epoch  11/300 Batch   40/390 - Batch Loss:  1.238, seconds: 59.91\n",
      "Average loss: 1.235\n",
      "Saving model\n",
      "** Epoch  11/300 Batch   60/390 - Batch Loss:  1.209, seconds: 104.58\n",
      "Average loss: 1.224\n",
      "Saving model\n",
      "** Epoch  12/300 Batch   20/390 - Batch Loss:  1.325, seconds: 57.03\n",
      "Average loss: 1.289\n",
      "No Improvement.\n",
      "** Epoch  12/300 Batch   40/390 - Batch Loss:  1.190, seconds: 60.23\n",
      "Average loss: 1.195\n",
      "Saving model\n",
      "** Epoch  12/300 Batch   60/390 - Batch Loss:  1.177, seconds: 103.88\n",
      "Average loss: 1.18\n",
      "Saving model\n",
      "** Epoch  13/300 Batch   20/390 - Batch Loss:  1.281, seconds: 57.25\n",
      "Average loss: 1.236\n",
      "No Improvement.\n",
      "** Epoch  13/300 Batch   40/390 - Batch Loss:  1.136, seconds: 61.37\n",
      "Average loss: 1.158\n",
      "Saving model\n",
      "** Epoch  13/300 Batch   60/390 - Batch Loss:  1.129, seconds: 104.50\n",
      "Average loss: 1.126\n",
      "Saving model\n",
      "** Epoch  14/300 Batch   20/390 - Batch Loss:  1.226, seconds: 59.54\n",
      "Average loss: 1.193\n",
      "No Improvement.\n",
      "** Epoch  14/300 Batch   40/390 - Batch Loss:  1.103, seconds: 60.40\n",
      "Average loss: 1.11\n",
      "Saving model\n",
      "** Epoch  14/300 Batch   60/390 - Batch Loss:  1.079, seconds: 104.15\n",
      "Average loss: 1.07\n",
      "Saving model\n",
      "** Epoch  15/300 Batch   20/390 - Batch Loss:  1.178, seconds: 56.92\n",
      "Average loss: 1.135\n",
      "No Improvement.\n",
      "** Epoch  15/300 Batch   40/390 - Batch Loss:  1.055, seconds: 60.36\n",
      "Average loss: 1.08\n",
      "No Improvement.\n",
      "** Epoch  15/300 Batch   60/390 - Batch Loss:  1.058, seconds: 103.98\n",
      "Average loss: 1.037\n",
      "Saving model\n",
      "** Epoch  16/300 Batch   20/390 - Batch Loss:  1.123, seconds: 56.80\n",
      "Average loss: 1.085\n",
      "No Improvement.\n",
      "** Epoch  16/300 Batch   40/390 - Batch Loss:  1.019, seconds: 59.78\n",
      "Average loss: 1.048\n",
      "No Improvement.\n",
      "** Epoch  16/300 Batch   60/390 - Batch Loss:  1.042, seconds: 103.13\n",
      "Average loss: 1.023\n",
      "Saving model\n",
      "** Epoch  17/300 Batch   20/390 - Batch Loss:  1.067, seconds: 56.62\n",
      "Average loss: 1.039\n",
      "No Improvement.\n",
      "** Epoch  17/300 Batch   40/390 - Batch Loss:  0.988, seconds: 60.33\n",
      "Average loss: 1.0\n",
      "Saving model\n",
      "** Epoch  17/300 Batch   60/390 - Batch Loss:  1.014, seconds: 103.96\n",
      "Average loss: 1.015\n",
      "No Improvement.\n",
      "** Epoch  18/300 Batch   20/390 - Batch Loss:  1.040, seconds: 56.82\n",
      "Average loss: 1.005\n",
      "No Improvement.\n",
      "** Epoch  18/300 Batch   40/390 - Batch Loss:  0.933, seconds: 59.99\n",
      "Average loss: 0.958\n",
      "Saving model\n",
      "** Epoch  18/300 Batch   60/390 - Batch Loss:  0.979, seconds: 103.65\n",
      "Average loss: 0.976\n",
      "No Improvement.\n",
      "** Epoch  19/300 Batch   20/390 - Batch Loss:  1.012, seconds: 56.56\n",
      "Average loss: 0.977\n",
      "No Improvement.\n",
      "** Epoch  19/300 Batch   40/390 - Batch Loss:  0.915, seconds: 59.80\n",
      "Average loss: 0.939\n",
      "Saving model\n",
      "** Epoch  19/300 Batch   60/390 - Batch Loss:  0.939, seconds: 103.82\n",
      "Average loss: 0.948\n",
      "No Improvement.\n",
      "** Epoch  20/300 Batch   20/390 - Batch Loss:  0.993, seconds: 56.96\n",
      "Average loss: 0.961\n",
      "No Improvement.\n",
      "** Epoch  20/300 Batch   40/390 - Batch Loss:  0.900, seconds: 60.76\n",
      "Average loss: 0.913\n",
      "Saving model\n",
      "** Epoch  20/300 Batch   60/390 - Batch Loss:  0.908, seconds: 104.41\n",
      "Average loss: 0.921\n",
      "No Improvement.\n",
      "** Epoch  21/300 Batch   20/390 - Batch Loss:  0.986, seconds: 56.97\n",
      "Average loss: 0.953\n",
      "No Improvement.\n",
      "** Epoch  21/300 Batch   40/390 - Batch Loss:  0.892, seconds: 60.21\n",
      "Average loss: 0.901\n",
      "Saving model\n",
      "** Epoch  21/300 Batch   60/390 - Batch Loss:  0.898, seconds: 105.74\n",
      "Average loss: 0.915\n",
      "No Improvement.\n",
      "** Epoch  22/300 Batch   20/390 - Batch Loss:  0.971, seconds: 57.30\n",
      "Average loss: 0.945\n",
      "No Improvement.\n",
      "** Epoch  22/300 Batch   40/390 - Batch Loss:  0.875, seconds: 60.29\n",
      "Average loss: 0.884\n",
      "Saving model\n",
      "** Epoch  22/300 Batch   60/390 - Batch Loss:  0.890, seconds: 104.57\n",
      "Average loss: 0.898\n",
      "No Improvement.\n",
      "** Epoch  23/300 Batch   20/390 - Batch Loss:  0.956, seconds: 56.54\n",
      "Average loss: 0.93\n",
      "No Improvement.\n",
      "** Epoch  23/300 Batch   40/390 - Batch Loss:  0.867, seconds: 60.31\n",
      "Average loss: 0.883\n",
      "Saving model\n",
      "** Epoch  23/300 Batch   60/390 - Batch Loss:  0.885, seconds: 103.72\n",
      "Average loss: 0.88\n",
      "Saving model\n",
      "** Epoch  24/300 Batch   20/390 - Batch Loss:  0.950, seconds: 56.70\n",
      "Average loss: 0.916\n",
      "No Improvement.\n",
      "** Epoch  24/300 Batch   40/390 - Batch Loss:  0.849, seconds: 60.59\n",
      "Average loss: 0.867\n",
      "Saving model\n",
      "** Epoch  24/300 Batch   60/390 - Batch Loss:  0.853, seconds: 104.60\n",
      "Average loss: 0.847\n",
      "Saving model\n",
      "** Epoch  25/300 Batch   20/390 - Batch Loss:  0.930, seconds: 57.30\n",
      "Average loss: 0.903\n",
      "No Improvement.\n",
      "** Epoch  25/300 Batch   40/390 - Batch Loss:  0.847, seconds: 60.27\n",
      "Average loss: 0.855\n",
      "No Improvement.\n",
      "** Epoch  25/300 Batch   60/390 - Batch Loss:  0.846, seconds: 104.29\n",
      "Average loss: 0.847\n",
      "No Improvement.\n",
      "Stopping Training.\n"
     ]
    }
   ],
   "source": [
    "min_learning_rate = 0.0006\n",
    "display_step = 20 \n",
    "stop_early_count = 0 \n",
    "stop_early_max_count = 3 \n",
    "per_epoch = 3 \n",
    "\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] \n",
    "\n",
    "en_train = en_filtered[0:5000]\n",
    "ml_train = ml_filtered[0:5000]\n",
    "update_check = (len(ml_train)//batch_size//per_epoch)-1\n",
    "checkpoint = logs_path + 'best_so_far_model.ckpt' \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    tf_summary_writer = tf.summary.FileWriter(logs_path, graph=train_graph)\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (en_batch, ml_batch, en_text_len, ml_text_len) in enumerate(\n",
    "                get_batches(en_train, ml_train, batch_size)):\n",
    "            before = time.time()\n",
    "            _,loss,summary = sess.run(\n",
    "                [train_op, tr_cost,merged_summary_op],\n",
    "                {input_data: ml_batch,\n",
    "                 targets: en_batch,\n",
    "                 learning_rate: lr,\n",
    "                 en_len: en_text_len,\n",
    "                 ml_len: ml_text_len,\n",
    "                 dropout_probs: dr_prob})\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            after = time.time()\n",
    "            batch_time = after - before\n",
    "            tf_summary_writer.add_summary(summary, epoch_i * batch_size + batch_i)\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('** Epoch {:>3}/{} Batch {:>4}/{} - Batch Loss: {:>6.3f}, seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(ml_filtered) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('Saving model') \n",
    "                    stop_early_count = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early_count += 1\n",
    "                    if stop_early_count == stop_early_max_count:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "\n",
    "        if stop_early_count == stop_early_max_count:\n",
    "            print(\"Stopping Training.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t694IbTCXWp6"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "id": "wGnUKPXoakw4",
    "outputId": "33978ef2-5339-454d-f431-84ca75cc7324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Malayalam\n",
      "  Word Ids:    [5038, 5039, 349, 5040, 5041, 5042, 5043, 5044, 1387, 3345, 735, 180, 5045, 5046, 9]\n",
      "  Input Words: ബുധൻ ശുക്രൻ ഭൂമി ചൊവ്വ വ്യാഴം ശനി യുറാനസ് നെപ്റ്റ്യൂൺ എന്നിങ്ങനെ എട്ടു ഗ്രഹങ്ങൾ സൂര്യനെ വലം വയ്ക്കുന്നു .\n",
      "\n",
      "English Text\n",
      "  Word Ids:       [7014, 22, 427, 22, 7632, 22, 7710, 22, 7711, 22, 7712, 22, 7713, 2306, 24]\n",
      "  Response Words: Mercury , Earth , Mars , Jupiter , Saturn , Uranus , Neptune rotate .\n",
      " Ground Truth: Mercury , Venus , Earth , Mars , Jupiter , Saturn , Uranus , Neptune are the eight planets that orbit the sun . <EOS>\n"
     ]
    }
   ],
   "source": [
    "#random = np.random.randint(3000,len(fr_filtered))\n",
    "random = np.random.randint(0,3000)\n",
    "ml_text = ml_filtered[random]\n",
    "\n",
    "checkpoint = logs_path + 'best_so_far_model.ckpt'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    input_data = loaded_graph.get_tensor_by_name('input_data:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    ml_length = loaded_graph.get_tensor_by_name('ml_len:0')\n",
    "    en_length = loaded_graph.get_tensor_by_name('en_len:0')\n",
    "    dropout_prob = loaded_graph.get_tensor_by_name('dropout_probs:0')\n",
    "    result_logits = sess.run(logits, {input_data: [ml_text]*batch_size, \n",
    "                                      en_length: [len(ml_text)], \n",
    "                                      ml_length: [len(ml_text)]*batch_size,\n",
    "                                      dropout_prob: 1.0})[0] \n",
    "\n",
    "pad = en_word2int[TOKEN_PAD] \n",
    "\n",
    "#print('\\nOriginal Text:', input_sentence)\n",
    "\n",
    "print('\\nMalayalam')\n",
    "print('  Word Ids:    {}'.format([i for i in ml_text]))\n",
    "print('  Input Words: {}'.format(\" \".join( [ml_int2word[i] for i in ml_text ] )))\n",
    "\n",
    "print('\\nEnglish Text')\n",
    "print('  Word Ids:       {}'.format([i for i in result_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join( [en_int2word[i]for i in result_logits if i!=pad] )))\n",
    "print(' Ground Truth: {}'.format(\" \".join( [en_int2word[i] for i in en_filtered[random]] )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9LluBaCvannd",
    "outputId": "b88bad58-58b2-4c65-8937-a6cbde0d5f01"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence_bleu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-07026c37f5f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence_bleu' is not defined"
     ]
    }
   ],
   "source": [
    "score = sentence_bleu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vs6VKEhFOrV2"
   },
   "outputs": [],
   "source": [
    "op = \" \".join( [en_int2word[i]for i in result_logits if i!=pad] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tYODMZKB10mW"
   },
   "outputs": [],
   "source": [
    "ip = \" \".join( [en_int2word[i] for i in en_filtered[random]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KTbys1Nrarp0",
    "outputId": "f9ae9a6f-54b6-45c0-d95f-3a3d5c19f5f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Male sexual organ has two important functions <EOS>'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RgOA8tx5D2Sw"
   },
   "outputs": [],
   "source": [
    "score = sentence_bleu(op,ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Do_1drAF2sG1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3634705916699639e-231"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow_nmt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
